#Assignment Name: HW 8
#Student Name: Mo Pei

# Problem One

Quakes <- read.csv("C:/Users/peimo/Desktop/MATH 156/Data/Quakes.csv")

#------------------------------------------------------------
# This function takes input shape parameter k and
# the data to compute
# (1/k)+ (1/n)*sum (log(xi)) -(1/alpha)sum xi^klog(xi)=0
# where alpha= sum xi^k.

weibull.shape <- function(k, data)
{
  numer <- colSums(outer(data, k, "^") * log(data))
  denom <- colSums(outer(data, k, "^"))
  numer/denom - 1/k - mean(log(data))
}

#-----
# This function takes input shape parameter k
# and data to compute
#  k^{th} root of (1/n) sum xi^k
# n=number of data values

weibull.scale <- function(k, data)
{
  mean(data^k)^(1/k)
}

##-----
# uniroot is a built-in R function which estimates the root
# of a function.
# Provide function, any arguments needed for function,
# and a guess of values two values around root.
# Function values must be opposite signs at lower
# and upper guess.

#Now, we do the data specific commands
timed <- Quakes$TimeDiff
#alternatively, wind <- subset(Turbine, select=AveSpeed, drop=TRUE)

uniroot(weibull.shape, data = timed, lower = 0.8,upper = 1)
# root, shape K  is 0.9172097

# With estimate of shape parameter, now find estimate
# of scale parameters

weibull.scale(0.9172097, timed)
# root, scale lambda is 17.346 

# Plot histogram with density curve overlap
# The prob=TRUE option scales histogram to area 1.

hist(timed, main = "Distribution of time difference",
     xlab = "days", prob = TRUE)
curve(dweibull(x, 0.9172097, 17.346), add = TRUE, col = "blue", lwd = 2)

dev.new()
plot.ecdf(timed,main = "ECDF of time difference data")
curve(pweibull(x,0.9172097,17.346), add=TRUE, col="blue",lwd=2)

# conclusion: we can see that the weibull distribution apprximates data
# pretty good!

library(stats4)
MLL <-function(lambda, k) -sum(dweibull(timed, k, lambda, log = TRUE))
mle(MLL,start = list(lambda = 10, k=0.5)) 
# shape      scale 
# 0.9171747 17.3432938 

# Problem Two

Service <- read.csv("C:/Users/peimo/Desktop/MATH 156/Data/Service.csv")

# (a)

wt <- Service$Times

Moment1 <- mean(wt); Moment1
Moment2 <- mean(wt^2); Moment2


variance <- Moment2 - Moment1^2 

# Gamma distribution
# e[x] = k*theta
# var[x] = k*theta^2

k<- Moment1^2/variance; k
# 2.670167
theta <- Moment1/k; theta
# 0.2602547

hist(wt, prob=TRUE, main = "MLE estimate of gamma distribution")
curve(dgamma(x, k, scale=theta), add=TRUE, col="blue",lwd=2)

MLL <-function(shape, scale) -sum(dgamma(wt, shape=shape, scale=scale, log = TRUE))
mle(MLL,start = list(shape = 1, scale = 1)) 
# shape     scale 
# 2.8127780 0.2470606 


# (b)

# Get the deciles
q <- qgamma(seq(.1, .9, by = .1), 2.81, scale=.247)
#range of wind
range(wt)
#encompass range of wind
q <- c(0, q, 2.2)
# Get the counts in each sub-interval. The plot=F command
# repeat above but store output
count <- hist(wt,breaks=q,plot=F)$counts
expected <- length(wt)*.1
# compute chi-square test statistic
ChiSq <- sum((count-expected)^2/expected)
pchisq(ChiSq, df =7, lower.tail = FALSE) #agrees with the P-value on page 146

# p value is over 0.90 and so we conclude that 
# the data from a gamma distribution with parameters shape 2.8127780 
# and scale 0.2470606. 

# (c) 

dev.new()
plot.ecdf(wt,main = "ECDF of Service Times")
curve(pgamma(x, k, scale=theta), add=TRUE, col="blue",lwd=2)

# Problem three

# (a)
score <- c(0.855, 0.891, 0.913, 0.989, 0.943)
theta <- -5/sum(log(score)); theta
# 11.55177

# (b)

# mean(score)=integral(x*theta*x^(theta-1))
# mean(score)=integral(theta*x^(theta))
# by integration
# mean(score)=theta/(1+ theta)
mean(score)
# 0.9182
# 0.9182=theta/(1+ theta)
# 0.9182=theta(1-0.9182)
# 0.9182=theta*0.0818
# 0.9182/0.0818=theta
# theta = 11.22494

# (c)
# likelihood for theta
# likelihood = Pai(f(Xi,thetha))
likelihood_fun <- function(theta)
{
  theta^5 * prod( score^(theta-1) )
}

#choose different theta
N <- 20; likelihood <- numeric(N)
for (i in 1:N) {
  likelihood[i] <- likelihood_fun(i)
}
plot(likelihood,xlab = 'Theta')
#MLE in vertical line
abline(v = 11.55177, col = "red")
# so likelihood values maximizes between 11 and 12. 
# This is consistent with MLE result.


# Problem Four

# (a)

lambda <- 5

N <- 100000; x_bar <- numeric(N)
for (i in 1:N) {
  x <- rexp(2, lambda)
  x_bar[i] <- mean(x)
}

# bias E[theta hat] - theta = 0 
bias <- mean(x_bar) - (1/lambda); bias
# 0.0005792527


# (d)

lambda <- 5

N <- 100000; x_s <- numeric(N)
for (i in 1:N) {
  x <- rexp(2, lambda)
  x_s[i] <- x[1:2]^0.5
}

# bias E[theta hat] - theta = 0 
bias <- mean(x_s) - (1/lambda); bias
# 0.1965748
# it is a biased estimate of theta



# Problem Five
install.packages("actuar")
library(actuar)
x <- rpareto(10, 1 , 2); x
# (a)
# consisent (1)if sample size goes to infinity, e(theta hat) = theta
#           (2)if sample size goes to infinity, variance(theta hat) = 0


plot(1, xlim= c(200, 20000), xlab='sample size',ylim = c(0, 250), log = "x", type = "n")
N = 1000;
for (n in c(250, 500, 1000, 2000, 5000, 10000, 20000)){
  means = numeric(N);
  for (i in 1:N){
    x <- rpareto(n,1, 2)
    means[i] = mean(x)
  }
  points( rep(n, N), means)
}

# conclusion: we can see that as sample size increases the mean of pareto distribution
# does not converge to anything. so it is not consistant to anything. 

# (b)

n <- 10; N = 1000; medians = numeric(N)  #much larger epsilon
#Repeat our procedure with the Cauchy distribution, whose expectation is undefined.
for (i in 1:N){
  x <- rpareto(n,1, 2)  #symmetric around theta = 5
  medians[i] = median(x)
}
hist(medians, breaks = 100, xlim = c(1, 20),prob=TRUE)  #lots of outliers are not shown
curve(dpareto(x,1,2), from = 1, to = 20,col='brown',add = TRUE)
mean(abs(medians -5) > epsilon) #miss the target most of the time

# (c)
# as we get more data, we expect an estimate become more accurate
# consisent (1)if sample size goes to infinity, e(theta hat) = theta
#           (2)if sample size goes to infinity, variance(theta hat) = 0


plot(1, xlim= c(200, 20000), ylim = c(0, 5), log = "x", type = "n")
s <- 2
epsilon <- .2
abline(h = c(s + epsilon, s- epsilon), col = "red")
N = 1000;  
for (n in c(250, 500, 1000, 2000, 5000, 10000, 20000)){
  medians = numeric(N);
  for (i in 1:N){
    x <- rpareto(n,1, s)
    medians[i] <- median(x)
  }
  points( rep(n, N), medians)
}


# we can see when sameple size increase from 250 to 20000
# median is a consistant estimate of scale s of pareto distribution of shape 1 and scale 2

